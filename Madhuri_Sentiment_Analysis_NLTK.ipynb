{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Libraries"
      ],
      "metadata": {
        "id": "9_tv5D9Hl6mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DHojWbmAr5ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset"
      ],
      "metadata": {
        "id": "m0gvkKVFl_Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('Restaurant_Reviews.tsv',delimiter=\"\\t\",quoting=3)"
      ],
      "metadata": {
        "id": "u-07KpJrsS_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "quoting=3 will ignore all the double quote in the dataset file"
      ],
      "metadata": {
        "id": "DUd8cxOJtYEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To upload a dataset\n",
        "\n",
        "1.   load the dataset in google drive\n",
        "2.   click on \"Files\" Icon on left hand side\n",
        "3.   Click on \"Permit access to google drive files\" on goole collab --> drive symbol should have cross on it --> should read unmount Drive\n",
        "4.   click on upload icon and select \"Restaurant_Reviews.tsv\" file\n"
      ],
      "metadata": {
        "id": "xnaXCJbOz0gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Texts"
      ],
      "metadata": {
        "id": "vG15qS9rmEXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus_review=[]  #list will hold all the cleaned review\n",
        "for i in range(0,1000):\n",
        "  review=re.sub('[^a-zA-Z]',' ',dataset['Review'][i]) \n",
        "  # replace all special symbols(non-letters) into space\n",
        "  review=review.lower()   #convert all reviews into lowercase\n",
        "  review=review.split()   #split words of reviews so stemming can be applied\n",
        "  ps=PorterStemmer()      #object of PorterStemmer() class\n",
        "  ####review=[ps.stem(w) for w in review if not w in set(stopwords.words('english'))]\n",
        "  # upperline will remove all stopwords including \"not\"--> so reviews wont have not word\n",
        "  #\"not\" word should not be included in stopwords because we need to keep not word in review\n",
        "  all_stopwords=stopwords.words('english')\n",
        "  all_stopwords.remove('not')\n",
        "  review=[ps.stem(w) for w in review if not w in set(all_stopwords)]\n",
        "  #apply stemming to each of the words in review which is not in stopword\n",
        "  #for w in review --> will iterate through each word in review and check if it is stopwords list\n",
        "  review=' '.join(review)\n",
        "  #join all the different words after stemming was applied to them\n",
        "  corpus_review.append(review)\n",
        "  #add each cleaned review to the corpus_review list\n",
        "  #\"not\" word should not be included in stopwords because we need to keep not word in review"
      ],
      "metadata": {
        "id": "9j8cYx300ssR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus_review)"
      ],
      "metadata": {
        "id": "C3-yyhMPPjKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "\n",
        "1.   nltk library will help us to remove all the stopwords \"The / a\" .. etc\n",
        "2.   from the corpus module of the nltk library import all the stopwords\n",
        "3.   PorterStemmer is used to apply stemming on our reviews\n",
        "4.   Stemming takes only the root of the word --> that indicates enough what the word means.\n",
        "5.   Sparse matrix contains all the different words of all the different reviews. Sparse matrix is created while creating bag of words model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eLfVBVlp4LaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Bag of Words Model"
      ],
      "metadata": {
        "id": "w0hfkBJ_4Be8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Sparse matrix will contain all the different reviews in different rows and all the different words taken from different reviews in different columns.\n",
        "1.   Each cell will either get 0 or 1. It will get zero if the word of the column is not in the review. It will get one if the word of the column is indeed part of the review of the row.\n",
        "2.   The process of creating columns of all of the words taken from all of the reviews is called tokenization.\n",
        "2.   bag of words model contains sparse matrix which contain all the words of the review after they were cleaned. All the different words of the review will be the columns of the sparse matrix\n",
        "3.  Sparse matrix will be the future matrix of features that will combine the dependent variable vector that contains binary outcome--> that will train future ML model(Naive Bayes Model) \n",
        "\n"
      ],
      "metadata": {
        "id": "0UdSn3H44GKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#tokenization will be done from feature_extraction module from sckit-learn by a class CountVectorizer\n",
        "cv=CountVectorizer(max_features=1500)\n",
        "#CountVectorizer --> takes one parameter max no of columns (words) ie max size of sparse matrix\n",
        "#CountVectorizer parameter takes most frequent words because some unnecessary words can be excluded\n",
        "#CountVectorier creates matrix of features ie sparse matrix\n",
        "X=cv.fit_transform(corpus_review).toarray()\n",
        "#fit_transform will fit the input of cv ie corpus to X\n",
        "#fit will take all the words and transform will put all the words into column of X\n",
        "y=dataset.iloc[:,-1].values\n",
        "#dependent variable y takes the last column from the dataset\n"
      ],
      "metadata": {
        "id": "U6mhwWbqLeKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X[0])"
      ],
      "metadata": {
        "id": "oZ_UWbLokO3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset into training and test set"
      ],
      "metadata": {
        "id": "UHNBG7_0mPjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "KE5C4_QRoSy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Naive Bayes model on the training set"
      ],
      "metadata": {
        "id": "hxCwswmwmS7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier=GaussianNB()\n",
        "classifier.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "d4abrCJSpX3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the test set results"
      ],
      "metadata": {
        "id": "mne1RoM6mali"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))\n"
      ],
      "metadata": {
        "id": "AJyLKuzbqdTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  y_pred=vector of prediction we got because of naive bayes\n",
        "2.   y_test=vector of real results containing the real outcome of the review\n",
        "\n"
      ],
      "metadata": {
        "id": "MBwmDBtuqFmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making the confusion matrix"
      ],
      "metadata": {
        "id": "cinhGTzhmgpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   accuracy_score is the number of correct predictions divided by the total number of observations in the test set\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "f70l2ZtprwWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "BV2QL1gZr_QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting if the single review is positive or negative"
      ],
      "metadata": {
        "id": "-GrnmnwPvItV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positive review"
      ],
      "metadata": {
        "id": "h0WqdsLfvUWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use our model to predict if the following review:\n",
        "\n",
        "\"I love this restaurant so much\"\n",
        "\n",
        "is positive or negative.\n",
        "\n"
      ],
      "metadata": {
        "id": "r8Ol3QErvZG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution: We just repeat the same text preprocessing process we did before, but this time with a single review."
      ],
      "metadata": {
        "id": "cNzIODcQvjTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_review = 'I love this restaurant so much'\n",
        "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
        "new_review = new_review.lower()\n",
        "new_review = new_review.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
        "new_review = ' '.join(new_review)\n",
        "new_corpus = [new_review]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier.predict(new_X_test)\n",
        "print(new_y_pred)\n"
      ],
      "metadata": {
        "id": "U2juJqMjvlV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The review was correctly predicted as positive by our model."
      ],
      "metadata": {
        "id": "vYOn4HGvvueW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative review"
      ],
      "metadata": {
        "id": "34u1LF5NwCty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative review\n",
        "Use our model to predict if the following review:\n",
        "\n",
        "\"I hate this restaurant so much\"\n",
        "\n",
        "is positive or negative.\n",
        "\n",
        "Solution: We just repeat the same text preprocessing process we did before, but this time with a single review."
      ],
      "metadata": {
        "id": "u_cTgFFawG42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_review = 'I hate this restaurant so much'\n",
        "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
        "new_review = new_review.lower()\n",
        "new_review = new_review.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
        "new_review = ' '.join(new_review)\n",
        "new_corpus = [new_review]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier.predict(new_X_test)\n",
        "print(new_y_pred)"
      ],
      "metadata": {
        "id": "28DMk_FpwLXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The review was correctly predicted as negative by our model."
      ],
      "metadata": {
        "id": "j-UNkoxewtMe"
      }
    }
  ]
}